\documentclass[a4paper,11pt,onecolumn,twoside]{article}
\usepackage{fancyhdr}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{fontspec}
\usepackage{times}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{subfigure}
\usepackage{float}
\usepackage{caption}
\usepackage{graphicx}
% Please change the following fonts if they are not available.
\addtolength{\topmargin}{-54pt}
\setlength{\oddsidemargin}{-0.9cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{17.00cm}
\setlength{\textheight}{24.50cm}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[colorlinks,linkcolor=black,anchorcolor=black,citecolor=black]{hyperref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
	numbers=left,
	numberstyle=\tiny,
	stringstyle=\color{purple},
	basicstyle=\footnotesize\ttfamily, 
	keywordstyle=\color{blue}\bfseries,
	commentstyle=\color{olive},
	frame=shadowbox,
	%framerule=0pt,
	%backgroundcolor=\color{pink},
	rulesepcolor=\color{red!20!green!20!blue!20},
	%rulesepcolor=\color{brown}
	%xleftmargin=2em,xrightmargin=2em,aboveskip=1em
	escapeinside=``, 
	basicstyle=\tiny
}
\renewcommand{\baselinestretch}{1.1}
\parindent 22pt

\title{\huge Analyzing Hospital Dataset Using Linear Regression }
\author{
Wangqian Miao\footnote{The two authors are all exchange students from Nanjing university}
\\[2pt]
{\large \textit{Kuang Yaming Hornors School, Biophysics, Nanjing University}}\\[6pt]
Mingyi Xue\\[2pt]
{\large \textit{School of Chemistry and Chemical Engineering, Nanjing University}}\\[6pt]
Instructor: Dr. Erin K. Melcon\\[2pt]
{\large \textit{Department of Statistics, University of California, Davis}}\\[2pt]
}
\date{}

\fancypagestyle{firststyle}
{
   \fancyhf{}
   \fancyhead[C]{STA101: Advanced Statistics For Biological Science, Course Project 1}
   \fancyhead[R]{\thepage}
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[CE]{STA101: Advanced Statistics For Biological Science}
\fancyhead[RE]{Project 1}
\fancyhead[CO]{W. Miao, M. Xue: Analyzing Hospital Dataset Using Linear Regression}
\fancyhead[LO]{}
\setlist{nolistsep}
\captionsetup{font=small}
\newcommand{\supercite}[1]{\textsuperscript{\cite{#1}}}
\begin{document}
\maketitle
\thispagestyle{firststyle}
\setlength{\oddsidemargin}{ 1cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{15.50cm}
\vspace{-.8cm}
\setcounter{page}{1}
\setlength{\oddsidemargin}{-.5cm}  % 3.17cm - 1 inch
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{17.00cm}
\tableofcontents
\newpage
\section{Introduction}
In this article, we use the linear regression model to analyze the dataset of ``Hospfull.csv" which describes characteristics of the United States Hospitals.
The source of this data is the text: ``Applied Linear Statistical Models, fifth edition, Kutner, Nachtsheim, Neter,
and Li."\par
Our goal is predicting the average estimated probability of acquiring infection in hospital (in percantage) by finding the important explanary variables. Depending on the tools and techniques we learn in linear regression, we will build a ``correct" model and make the prediction. \par
Firstly, we will use the full model and make the improvement later. In the dataset,
we choose ``Infect'' as our response variable $Y$ and let other variables be the explanary variables $X_i$. So, the linear regression model is:
\begin{equation}
Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\varepsilon
\end{equation}
We make a summary table as following to intepret the variables. 
 \begin{table}[htbp]
 	\centering
 	\begin{tabular}{cccc}
 		\midrule[1.5pt]
 		Name& Variable &Variable Kind  & Units\\
 		\hline
 		Indfect&$Y$ & Response  & Percentage\\
 	    Length& $X_1$& Numerical  & Days  \\
 		 Culture& $X_2$ &Numerical & Ratio \\
 		 Bed& $X_3$ & Numerical& Number \\
 		 Medschool&$X_4$& Categorical  & Y/N \\
 		Region& $X_5$& Categorical  & NE/NC/S/W \\
 		\midrule[1.5pt]
 	\end{tabular}
 	\caption{A summary table for the variables }
 \end{table}
\section{Summary}

\subsection{Analyzing the Sample Correlation Coeffient}
Firstly,we analyze the correlaytion coeffient between $Y$ and numerical variables to find whether the linear relationship between $Y$ and $X_i$ is strong enough.
 \begin{table}[htbp]
	\centering
	\begin{tabular}{ccc}
		\midrule[1.5pt]
		 $Y$ v.s. $X_i$&Correlation Coeffient & Linear Relationship Strength\\
		\hline
	    $X_1$ &  0.5334& Moderate \& Positive\\
		$X_2$ &  0.5592 &Moderate \& Positive  \\
		$X_3$ &  0.3598& Weak \& Positive  \\
		\midrule[1.5pt]
	\end{tabular}
	\caption{A summary table for the variables }
\end{table}
\subsection{Scatter Plot}

\begin{figure}[H]
	\centering
	\subfigure{
		\begin{minipage}[b]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{scatter_plot_bed.png} \\
			\includegraphics[width=1\textwidth]{scatter_plot_culture.png} 
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[b]{0.55\textwidth}
			\includegraphics[width=1\textwidth,height=0.3\textheight]{scatter_plot_length.png} \\
		\end{minipage}
	}
\caption{Scatter plots by different numerical variables}
\end{figure}
\subsection{Five Number Summary}
 \begin{table}[H]
	\centering
	\begin{tabular}{ccccccc}
		\midrule[1.5pt]
        Region &Min &$Q_1$ &Median&Mean &$Q_3$ &Max\\
        \hline
       NC   & 1.300       &3.8500     &4.400   &4.394      &5.225   &7.800\\
      NE    &2.500       &4.200     &4.850   &4.861      &5.750  &7.700\\
       S    &1.300       &2.900      &4.200    &3.927       &4.700  &7.600\\
      W    &2.600       &4.075      &4.450   &4.381      &4.850    &5.600\\
        
		\midrule[1.5pt]
	\end{tabular}
	\caption{A summary table for the variables }
\end{table}
 \begin{table}[H]
	\centering
	\begin{tabular}{ccccccc}
		\midrule[1.5pt]
		Medschool &Min &$Q_1$ &Median&Mean &$Q_3$ &Max\\
		\hline
    N    &1.300       &3.400     &4.300  &4.224      &5.025    &7.800000\\
     Y    &2.900     & 4.500    & 5.000    &5.094       &5.700    &7.700\\
		\midrule[1.5pt]
	\end{tabular}
	\caption{A summary table for the variables }
\end{table}

\section{Data Preparation}
\subsection{Boxplots}
According to the Boxplot by different catogorical, we find there exists some outliers in the dataset.
\begin{figure}[H]
	\centering
	\subfigure[the first subfigure]{
		\begin{minipage}[b]{0.43\textwidth}
			\includegraphics[width=1\textwidth]{group_boxplot_medschool.png} \\
		\end{minipage}
	}
	\subfigure[the second subfigure]{
		\begin{minipage}[b]{0.43\textwidth}
			\includegraphics[width=1\textwidth]{group_boxplot_region.png} \\
		\end{minipage}
	}
\end{figure}

\subsection{Remove Outliers}
The outliers are shown in the following table. We remove 10 outliers and the ratio of the full dataset is 8.850\%.
 \begin{table}[H]
	\centering
	\begin{tabular}{ccccccc}
		\midrule[1.5pt]
		Index & Length &Infect &Culture &Bed &MedSchool& Region\\
		\hline
	
        2    &8.82    &1.6     &3.8  &80         &N     &NC\\
		8  &11.18    &5.4    &60.5 &640         &Y     &NC\\
	    13  &12.78    &7.7      &46 &322         &Y    &NE\\
	    47   &19.56   & 6.5    &17.2 &306         &N     &NE\\
	    53  &11.41    &7.6    &16.6 &535         &N      &S\\
	    54  &12.07   & 7.8    &52.4 &157         &N     &NC\\
	    93   &8.92   & 1.3     &2.2  &56         &N     &NC\\
	 	101  & 9.76  &  2.6     &6.9  &64         &N      &W\\
	    103   &7.14  &  2.7    &13.1  &92        & N      &W\\
	    112  &17.94  &  5.9    &26.4 &835         &Y     &NE\\
		\midrule[1.5pt]
	\end{tabular}
	\caption{The information of outliers }
\end{table}
\section{Model Fitting}
\subsection{Using A.I.C \& B.I.C}
Based on the criatia of A.I.C or B.I.C, the ``correct" model can be found. \par
Without considering the interaction term, we find that the full model has the lowest A.I.C. based on the all subsets of the full model.The model we build is
\begin{equation}
\begin{split}
\hat{y_1}= &-0.4536+0.3962X_1+0.0586X_2+0.0013X_3-0.4005X_{4Y}\\ &-0.4577X_{5NE}-0.3619X_{5S}+0.9163X_{5W}
\end{split}
\end{equation} \par
Without considering the interaction term, we find that the full model has the lowest B.I.C. based on the all subsets of the full model.The model we build is
\begin{equation}
\begin{split}
\hat{y_2}= &-0.5573+0.4389X_1+0.0565X_2\\&-0.5097X_{5NE}     -0.3471X_{5S}+0.9048X_{5W} 
\end{split}
\end{equation}
There are some differences between the two models, we will use some techniques to compare them.
\subsection{C.I. \& H.T. for $\beta_i$}
We analyze the full model's $\beta s$ C.I.($\alpha=0.5$) for the first model. The table is as following.
\begin{table}[H]
	\centering
	\begin{tabular}{ccc}
		\midrule[1.5pt]
		& 2.5 \% &97.5 \% \\
		\hline
		(Intercept) &-1.8014 &0.8942\\
		$X_1$         &  0.2507 &0.5417\\
		$X_2$           &0.0371 &0.0802\\
		$X_3$           &0.0001 &0.0024\\
		$X_{4Y}$         &-0.9805 &0.1796\\
		$X_{5NE}$        &-0.9469 &0.0316\\
		$X_{5S}$         &-0.7823 &0.0585\\
		$X_{5W}$          &0.3422 &1.4905\\	
		\midrule[1.5pt]
	\end{tabular}
	\caption{C.I. of $\beta s$ of the first model }
\end{table}
The C.I. of $X_4$ contains 0. We will consider that $X_4$ should be dropped. 
\begin{table}[H]
	\centering
	\begin{tabular}{ccccc}
		\midrule[1.5pt]	
		&Coefficients:
		&Estimate Std. Error &$t$ value &Pr($>|t|$)\\
		\hline    
	  (Intercept) &-0.453605 &  0.678888 & -0.6680 &0.50565    \\
		$X_1$          & 0.396160  & 0.073290   &5.405 &4.79e-07\\
		$X_2$          & 0.058650  & 0.010860   &5.401 &4.89e-07\\
		$X_3$          & 0.001265   &0.000568   &2.227  &0.02833 \\
		$X_{4Y}$         &-0.400484   &0.292175  &-1.371  &0.17370  \\  
		$X_{5NE}$        &-0.457657  & 0.246435  &-1.857  &0.06639 \\
		$X_{5S}$         &-0.361916   &0.211761  &-1.709  &0.09070 \\
		$X_{5W}$         & 0.916322   &0.289204   &3.168  &0.00206 \\
		\midrule[1.5pt]
	\end{tabular}
	\caption{H.T. for $\beta s$ of the first model }
\end{table}
The p-value of $\beta_4$ is big enough for us to accept $H_0$ which means $\beta_4=0$, we can drop $X_4$. Now we have a model `` $Y~X_1,X_2,X_3,X_5$". We find that $\beta_3's$ value is quite small. How much error we will reduce by adding $X_3$ in our model ``$Y\sim X_1,X_2,X_5$" ?  
\subsection{Partial $R^2$}
We use partial $R^2$ to estimate how much error we will reduce by adding $X_3$.
\begin{equation}
R^2=\frac{SSE_{S}-SSE_{L}}{SSE_{S}}=3.156\%
\end{equation}
Our goal is to find a ``correct" model, so we can drop $X_3$ and choose the second model $Y\sim X_1,X_2,X_5$ which is the best under the B.I.C criteria.
\subsection{Add Interavtion Term}
At last we consider that wether we should add the interaction term analyzing the C.I. of $\beta s$, we find there is no need to add interaction term, because the C.I. all contain 0.
\subsection{Model Selection}
Based on the discussion above, we find the most important variables $X_1,X_2,X_5$ and our final model is that
\begin{equation}
\begin{split}
Y\sim X_1+X_2+X_5
\end{split}
\end{equation}
\section{Model Diagnotics}
There are some assumptions we should obey by using linear regression. Mostly, we care about the normality of $e_i$ and whether the variance is a constant.
\subsection{$e_i$ Normality}
The assumption is that $\varepsilon\sim \mathcal N(0,\sigma_{\varepsilon})$. We use $e_i$ to estimate $\varepsilon$ in practice. There are two popular ways which are QQ plot and Shapiro-Wilk Normality Test.
\subsubsection{QQ Plot}
Accoding to the figure below, most points are near to $y=x$ which suggests $e_i$ is normarly distributed.
 \begin{figure}[H]
	\centering
	\includegraphics[width=.65\textwidth,height=.35\textheight]{qqplot.png} %1.png是图片文件的相对路径
	\caption{best} %caption是图片的标题
\end{figure}
\subsubsection{Shapiro-Wilk Normality Test}
We use Shapiro-Wilk Test to find whether the error is normaly distributed.
\begin{itemize}
	\item $H_0$: The error is normaly distributed.
	\item $H_A$: The error is not normaly distributed.
\end{itemize}
Accoding to R, the p-value=0.5181. Is is large enough for us to accept null which
means the error is normaly distributed under any significance.
\subsubsection{Conclusion}
Our model obey the assumption that $\varepsilon\sim \mathcal N(0,\sigma_{\varepsilon})$.  
\subsection{Constant Variance}
The assumption is that $\sigma_{\varepsilon}$ is constant. There are two popular ways which are plotting $e_i v.s. \hat{y_i}$ and Fligner-Killeen  Test.
\subsubsection{$e_i$ v.s. $\hat{y_i}$ Plot}
Accoding to the figure below , there is no clear pattern in the plot, so the conclusion is that the variance is constant.
 \begin{figure}[H]
	\centering
	\includegraphics[width=.65\textwidth,height=.30\textheight]{scatter_plot_constant_variance.png} %1.png是图片文件的相对路径
	\caption{best} %caption是图片的标题
	\label{img} %此处的label相当于一个图片的专属标志，目的是方便上下文的引用
\end{figure}
\subsubsection{Fligner-Killeen  Test}
We use Fligner-Killeen Test to find whether the variance is constant.
\begin{itemize}
	\item $H_0$: $\sigma^2_{lower}=\sigma^2_{higher}$.
	\item $H_A$: $\sigma^2_{lower}\neq\sigma^2_{higher}$.
\end{itemize}
Accoding to R, the p-value=0.2361. Is is large enough for us to accept null which
means the variance is constant under any significance.
\subsubsection{Conclusion}
The conclusion is that our model obey the assumption that the variance is constant.
\subsection{Remove  Outliers Again}
We use the method of ``Cooks Distance" to find wether there still exists some outliers in the dataset. As shown in the figure below, The ``Cooks Distance" is so small that there is no need to move any points out of our dataset.
 \begin{figure}[H]
	\centering
	\includegraphics[width=.65\textwidth,height=.35\textheight]{cooks_distance.png} %1.png是图片文件的相对路径
	\caption{best} %caption是图片的标题
	\label{img} %此处的label相当于一个图片的专属标志，目的是方便上下文的引用
\end{figure}
\subsection{Final model}
Our final model is a ``correct"" model and it is appropriate.
\begin{equation}
\begin{split}
Y= &-0.5573+0.4389X_1+0.0565X_2-0.5097X_{5NE}\\&-0.3471X_{5S}+0.9048X_{5W} +\varepsilon
\end{split}
\end{equation}
\section{Interpretation}
In this section, we will interpret the meaning of $\beta s$ according to this problem.
\begin{itemize}
	\item $\beta_{0}$: Since the probability of acquring infection in hospital cannot be negative, it is inappropriate to predict the probability of acquiring infection at all $X$'s equal 0.
	\item $\beta_1$: When the length of stay of all patients in the hospital increases by 1 day, the probability of acquring infection in hospital tends to increase by 0.4389 percentage on average, holding all other variables constant.
	\item $\beta_2$: When the ratio of number of cultures performed to number of patients increases by 1, the probability of acquring infection in hospital tends to increase by 0.05648 percentage on average, holding all other variables constant.
	\item $\beta_{3}$: The probability of acquring infection in hospital tends to decrease by 0.5097 percentage on average when patients are in category of North East compared to patients in category of North Central, holding all other variables constant.
	\item $\beta_{4}$: The probability of acquiring infection in hospital tends to decrease by 0.3471 percentage on average when patients are in category of South compared to patients in category of North Central, holding all other variables constant.
	\item $\beta_{5}$: bThe probability of acquiring infection in hospital tends to increase by 0.9048 percentage on average when patients are in category of West compared to patients in category of North Central, holding all other variables constant.
	
\end{itemize}
\section{Prediction}
We will use our model to do a prediction.b
\section{Conclusion}
\section*{R Appendix}				
\begin{lstlisting}[language=R,caption={R script for Project 1}] 
##### set work directory and load dataset ##### 
setwd("/home/xmy/STA 101/Projects/P1")
HospFull<-read.csv("HospFull.csv", header = TRUE)
head(HospFull, n = 3)

##### load packages #####
library("ggplot2")
library("leaps")
library("MPV")

##### define functions #####
Partial.R2 = function(small.model,big.model){
SSE1 = sum(small.model$residuals^2)
SSE2 = sum(big.model$residuals^2)
PR2 = (SSE1 - SSE2)/SSE1
return(PR2)
}
All.Criteria = function(the.model){
p = length(the.model$coefficients)
n = length(the.model$residuals)
the.BIC = BIC(the.model)
the.LL = logLik(the.model)
the.AIC = AIC(the.model)
the.PRESS = PRESS(the.model)
the.R2adj = summary(the.model)$adj.r.squared
# the.CP = summary(the.model)$cp
the.results = c(the.LL,p,n,the.AIC,the.BIC,the.PRESS,the.R2adj)
names(the.results) = c("LL","p","n","AIC","BIC","PRESS","R2adj")
return(the.results)
}


##### correlation #####
cor(HospFull$Length, HospFull$Infect)
cor(HospFull$Culture, HospFull$Infect)
cor(HospFull$Bed, HospFull$Infect)


##### Infect summary #####
summary(HospFull$Infect)
# grouped by MedSchool
aggregate(Infect ~ MedSchool, data = HospFull, summary)
# grouped by Region
aggregate(Infect ~ Region, data = HospFull, summary)

# plot(HospFull)
##### boxplots of Infect #####
require(ggplot2)
# boxplot grouped by MedSchool
ppi = 600
# Calculate the height and width (in pixels) for a 4x3-inch image at 600 ppi
png("group_boxplot_medschool.png", width=6*ppi, height=4*ppi, res=ppi)
ggplot(HospFull, aes(y=Infect, x = MedSchool))+ theme_gray() + geom_boxplot() + ylab("Probability of acquiring infection in hospital") + 
xlab("category of MedSchool")+ coord_flip() 
#ggtitle("Boxplot of Infect grouped by Medchool") 
dev.off()

# boxplot grouped by Region
png("group_boxplot_region.png", width=6*ppi, height=4*ppi, res=ppi)
ggplot(HospFull, aes(y=Infect, x = Region))+ theme_gray() + geom_boxplot() + ylab("Probability of acquiring infection in hospital") + 
xlab("category of Geographical region")+ coord_flip() 
#ggtitle("Boxplot of Infect grouped by Region") 
dev.off()


##### scatter plots of Infect #####
# scatter plot of Infect vs. Length
png("scatter_plot_length.png", width=6*ppi, height=4*ppi, res = ppi)
qplot(HospFull$Length, HospFull$Infect, data = HospFull) +xlab("length of stay") + ylab("probability of acquiring infection")
dev.off()

# scatter plot of Infect vs. Culture
png("scatter_plot_culture.png",  width=6*ppi, height=4*ppi, res = ppi)
qplot(HospFull$Culture, HospFull$Infect, data = HospFull) +xlab("culture/patients * 100") + ylab("probability of acquiring infection")
dev.off()

# scatter plot of Infect vs. Bed
png("scatter_plot_bed.png", width=6*ppi, height=4*ppi, res = ppi)
qplot(HospFull$Bed, HospFull$Infect, data = HospFull) +xlab("number of beds") + ylab("probability of acquiring infection")
dev.off()



##### remove outliers according to plots #####
# cover HospFull
the.original = HospFull
HospFull=HospFull[-which(HospFull$Length>15),]
HospFull=HospFull[-which(HospFull$Culture>60),]
HospFull=HospFull[-which(HospFull$MedSchool=="Y" & HospFull$Infect > 7),]
HospFull=HospFull[-which(HospFull$MedSchool=="N" & HospFull$Infect > 7),]
HospFull=HospFull[-which(HospFull$Region=="W" & HospFull$Infect < 3),]
HospFull=HospFull[-which(HospFull$Region=="NC" & HospFull$Infect < 2),]
length(the.original$Infect)
length(HospFull$Infect)
the.ratio = (length(the.original$Infect)-length(HospFull$Infect))/length(the.original$Infect)
the.ratio

##### subset models of Infect~. #####
# rename dataset for convenience
names(HospFull) = c("X1","Y","X2","X3","X4","X5")
full.model = lm(Y~ X1 + X2 + X3 + X4 + X5,data = HospFull)
round(full.model$coefficients,4)
bic.model = lm(Y~X1+X2+X5, data = HospFull)
round(bic.model$coefficients, 4)
all.models = c("Y~1","Y~X1","Y~X2","Y~X3","Y~X4","Y~X5",
"Y~X1+X2","Y~X1+X3","Y~X1+X4","Y~X1+X5","Y~X2+X3","Y~X2+X4","Y~X2+X5","Y~X3+X4","Y~X3+X5","Y~X4+X5",
"Y~X1+X2+X3","Y~X1+X2+X4","Y~X1+X2+X5","Y~X1+X3+X4","Y~X1+X3+X5","Y~X1+X4+X5","Y~X2+X3+X4","Y~X2+X3+X5","Y~X2+X4+X5","Y~X3+X4+X5",
"Y~X1+X2+X3+X4","Y~X1+X2+X3+X5","Y~X1+X2+X4+X5","Y~X1+X3+X4+X5","Y~X2+X3+X4+X5",
"Y~X1+X2+X3+X4+X5")
Infect.all.model.crit = t(sapply(all.models,function(M){
current.model = lm(M,data = HospFull)
All.Criteria(current.model)
}))
Infect.all.model.crit
Infect.all.model.crit = data.frame(Infect.all.model.crit)
# find the model with lowest BIC
Infect.all.model.crit[which(Infect.all.model.crit$BIC == min(Infect.all.model.crit[,5])),]
# find the model with lowest AIC
Infect.all.model.crit[which(Infect.all.model.crit$AIC == min(Infect.all.model.crit[,4])),]


##### anova analysis of X4 #####
summary(full.model)
summary(bic.model)
alpha = 0.05
the.CIs = confint(full.model,level = 1-alpha)
round(the.CIs, 4)
# drop X4
smaller.model = lm(Y~X1+X2+X3+X5, data = HospFull)
anova.small = anova(smaller.model)
larger.model = lm(Y~X1+X2+X3+X4+X5, data = HospFull)
anova.large = anova(larger.model)
anova(smaller.model,larger.model)


##### anova analysis of X3 #####
smaller.model = lm(Y~X1+X2+X5, data = HospFull)
anova.small = anova(smaller.model)
larger.model = lm(Y~X1+X2+X3+X5, data = HospFull)
anova.large = anova(larger.model)
anova(smaller.model,larger.model)
##### partial r2 of X3 #####
partial.R2=Partial.R2(smaller.model, larger.model)
partial.R2


##### considering interaction terms #####
# interaction term between X1 and X5
final.model = lm(Y~X1+X2+X5, data = HospFull)
final.model
X1.interation.model = lm(Y~X1+X2+X5+X1*X5, data = HospFull)
summary(X1.interation.model)
confint(X1.interation.model,level = 1-alpha)
anova(final.model,X1.interation.model)
partial.R2=Partial.R2(final.model,X1.interation.model)
partial.R2
# interaction term between X2 and X5
X2.interation.model = lm(Y~X1+X2+X5+X2*X5, data = HospFull)
X2.interation.model
summary(X2.interation.model)
confint(X2.interation.model,level = 1-alpha)
anova(final.model,X2.interation.model)
partial.R2=Partial.R2(final.model,X2.interation.model)
partial.R2


##### diagnose of model #####
final.model = lm(Y~X1+X2+X5, data = HospFull)
final.model
HospFull$ei = final.model$residuals
HospFull$yhat = final.model$fitted.values
## nomality
# qqplot
png("qqplot.png", width=6*ppi, height=4*ppi, res = ppi)
qqnorm(final.model$residuals)
qqline(final.model$residuals)
dev.off()
# S-W test
the.SWtest = shapiro.test(final.model$residuals)
the.SWtest

## constant variance
# ei-yi plot
png("scatter_plot_constant_variance.png", width=6*ppi, height=4*ppi, res = ppi)
qplot(yhat, ei, data = HospFull) +
xlab("Fitted Values") + ylab("Errors") + geom_hline(yintercept = 0,col = "purple")
dev.off()
# F-K test
HospFull$ei = final.model$residuals
Group = rep("Lower",nrow(HospFull))
Group[HospFull$Y < median(HospFull$Y)] = "Upper"
Group = as.factor(Group)
HospFull$Group = Group
the.FKtest= fligner.test(HospFull$ei, HospFull$Group)
the.FKtest

## outliers
# cook's distance
cutoff = 0.10
CD = cooks.distance(final.model)
HospFull$CD = cooks.distance(final.model)
HospFull[which(HospFull$CD>cutoff),] 
# no outliers
png("cooks_distance.png", width=6*ppi, height=4*ppi, res = ppi)
plot(CD,ylab = "Cook's Distance")
abline(h = cutoff,color = "purple")
dev.off()

SR = stdres(final.model)
HospFull$SR = SR
cutoff= 3
png("standardized_error.png", width=6*ppi, height=4*ppi, res = ppi)
ggplot(HospFull,aes(x = SR))+geom_histogram(binwidth = 0.5,color = "black",fill = "white")+ xlab("standardized error")
dev.off()
SR[which(abs(SR) > cutoff)] 

##### final model #####
final.model
R2 = summary(final.model)$r.squared
R2

##### predict estimated values of Y #####
alpha = 0.05
x.star = data.frame(X1 = 8, X2 = 14, X5 = "W")
predict(final.model, x.star, interval = "confidence", level = 1-alpha)
predict(final.model, x.star, interval = "prediction", level = 1-alpha)
\end{lstlisting} 



\end{document}